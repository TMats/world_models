{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "World Models Library (OSS)",
      "provenance": [
        {
          "file_id": "15SeiOabjQkKUIN0rrNl76tuk-RxpnGRd",
          "timestamp": 1602641068192
        }
      ],
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL227x1Fox0u"
      },
      "source": [
        "# World Models Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnsUwXPopVUd"
      },
      "source": [
        "World Models Library is a pure python tool facilitating planning from pixels. The major components are the following:\n",
        "*   **World Model:** A *world model* is an stateless model of the environment that is used for planning. Any world model should implement the following three methods:\n",
        "  *   `reset_fn(**kwargs) -> state`. This function is responsible for resetting the state of the world model and is called at the beginning of every episode. The *state* can have any structure.\n",
        "  *   `observe(last_frames, last_actions, last_rewards, state) -> state`. This function is responsible for updating the state of the world model given the latest observed changes in the environment.\n",
        "  *   `predict_fn(future_actions, state) -> predictions`. This function predicts the future and will be used by planners to evaluate action proposals. The *predictions* should be compatible with the `objective_fn` that is passed to the planner. This function should not change the input *state*.\n",
        "\n",
        "  Any state that is required for the operation of the model (e.g. recurrent layer state) should be a part of `state` object that is passed around and returned from `observe_fn`. \n",
        "*   **Planner:** A *planner* decides what actions to take next with the help of a *world model*. The planner is responsible to keep track of the world model's *state* and call `reset_fn`, `observe_fn` and `predict_fn` to interact with the *world model*.\n",
        "*   **Task:** A *task* is a thin wrapper around `gym.Env` that adds a name to the underlying environment and provides convinient factory methods to instantiate environments.\n",
        "\n",
        "![World Models Library Components](https://i.imgur.com/JmuCcRI.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmWvr-VW_yak"
      },
      "source": [
        "## Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5340xDeR0_-F",
        "cellView": "both"
      },
      "source": [
        "#@title Imports\n",
        "import tensorflow.compat.v1 as tf\n",
        "from world_models.simulate import simulate\n",
        "from world_models.agents import planet\n",
        "from world_models.planners import planners\n",
        "from world_models.objectives import objectives\n",
        "from world_models.simulate import simulate\n",
        "from world_models.utils import npz\n",
        "from world_models.loops import train_eval\n",
        "from world_models.tasks import tasks\n",
        "\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKqMAbryQGyt"
      },
      "source": [
        "## Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evdKt-cxQL45"
      },
      "source": [
        "As a first step, we should choose a task to solve. There are several task suites already defined in the `tasks/tasks.py` file. We will use DeepMind Control's Cheetah task as example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "236d2dUpRtsm"
      },
      "source": [
        "task = tasks.DeepMindControl(domain_name='cheetah',\n",
        "                             task_name='run',\n",
        "                             action_repeat=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SJQA9W8MB0i"
      },
      "source": [
        "## World Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KdmiBUSMGZ7"
      },
      "source": [
        "There are already a few options available defined in the `agents` folder including *PlaNet*, *SV2P*, etc. For this colab we will instantiate a *PlaNet* agent. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if9Xmh_jJEhx"
      },
      "source": [
        "model = planet.RecurrentStateSpaceModel(task=task)\n",
        "model_dir = '/tmp/experiment/model'\n",
        "dist_strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "reset_fn = planet.create_planet_reset_fn(model=model)\n",
        "observe_fn = planet.create_planet_observe_fn(model=model,\n",
        "                                             model_dir=model_dir,\n",
        "                                             strategy=dist_strategy)\n",
        "predict_fn = planet.create_planet_predict_fn(model=model,\n",
        "                                             strategy=dist_strategy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bDItvH82nPW"
      },
      "source": [
        "In addition to `reset_fn`, `observe_fn` and `predict_fn`, we also need to define a `train_fn` as an extra hook to train the model on the latest collected episodes, with this signature: `train_fn(data_path) -> None`. There are utility functions for fast data processing in the `utils/npz.py` that can be used in a training loop but the library is agnostic to how training/checkpointing/restoring is done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTXpsZUN4RkO"
      },
      "source": [
        "train_steps = 100  # How many training steps per episode\n",
        "batch = 50 \n",
        "duration = 50  # How many timesteps in a single training sequence\n",
        "learning_rate = 1e-3\n",
        "\n",
        "train_fn = planet.create_planet_train_fn(model=model,\n",
        "                                         train_steps=train_steps,\n",
        "                                         batch=batch,\n",
        "                                         duration=duration,\n",
        "                                         learning_rate=learning_rate,\n",
        "                                         model_dir=model_dir,\n",
        "                                         strategy=dist_strategy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N93e5Abf9dW7"
      },
      "source": [
        "## Planner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIkpW1KE9gCh"
      },
      "source": [
        "The *planner* is responsible for decision making. It can use the world model to make predictions about the future and make informed decisions about which actions to take next. We normally need **separate planners** for training and evaluation, since we might need some sort of exploration during training that is not applicable for evaluation. We implemented a few planners in the `planners/planners.py` file including continous and discrete *Cross Entropy Method (CEM)*. Diagram below, shows how CEM iteratively refines itself to choose an optimal action.\n",
        "![CEM iteration](https://i.imgur.com/2iUmnIK.png)\n",
        "\n",
        "A planner also needs an `objective_fn` to compute scores from *world model* predictions. If a world model predicts rewards directly, we can use a `DiscountedReward` objective."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5aj1zWBAY5g"
      },
      "source": [
        "objective_fn = objectives.DiscountedReward()\n",
        "\n",
        "horizon = 12  # CEM planning horizon\n",
        "iterations = 10  # CEM iterations\n",
        "proposals = 1000  # Number of proposals to evaluate per iteration\n",
        "top_fraction = 0.1  # Fraction of proposals with highest scores for fitting\n",
        "\n",
        "# Base CEM planner to use for evaluation.\n",
        "base_cem = planners.CEM(predict_fn=predict_fn,\n",
        "                        observe_fn=observe_fn,\n",
        "                        reset_fn=reset_fn,\n",
        "                        task=task,\n",
        "                        objective_fn=objective_fn,\n",
        "                        horizon=horizon,\n",
        "                        iterations=iterations,\n",
        "                        proposals=proposals,\n",
        "                        fraction=top_fraction)\n",
        "\n",
        "# Training CEM planner with initial random cold start and random noise.\n",
        "# Pure random actions for the first `n` episodes to bootstrap the world model.\n",
        "random_cold_start_episodes = 5\n",
        "train_cem = planners.RandomColdStart(task=task,\n",
        "                                     random_episodes=random_cold_start_episodes,\n",
        "                                     base_planner=base_cem)\n",
        "# Add some Gaussian noise for active exploration.\n",
        "noise_scale = 0.3\n",
        "train_cem = planners.GaussianRandomNoise(task=task,\n",
        "                                         stdev=noise_scale,\n",
        "                                         base_planner=train_cem)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kysNYc7H2v-"
      },
      "source": [
        "## Simulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDRfdPglH9I2"
      },
      "source": [
        "In order to run an agent on the task, we can use the `simulate` function in the `simulate/simulate.py` file. Below is a diagram of the chain of events during an episode. \n",
        "\n",
        "![Simulation logic](https://i.imgur.com/JjwNHwj.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujXqWewrKZa5"
      },
      "source": [
        "episode_num = 0\n",
        "train_data_dir = '/tmp/experiment/data/train'\n",
        "train_summary_dir = '/tmp/experiment/train'\n",
        "episodes = list()\n",
        "\n",
        "for i in range(random_cold_start_episodes):\n",
        "  episode, predictions, score = simulate.simulate(task=task,\n",
        "                                                  planner=train_cem,\n",
        "                                                  num_episodes=1)\n",
        "  scalar_summaries = {'score': score}\n",
        "  train_eval.visualize(summary_dir=train_summary_dir,\n",
        "                       global_step=i,\n",
        "                       episodes=episode,\n",
        "                       predictions=predictions,\n",
        "                       scalars=scalar_summaries)\n",
        "  episodes.extend(episode)\n",
        "  episode_num += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-vYTuBhAoVN"
      },
      "source": [
        "%tensorboard --logdir=/tmp/experiment/ --port=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiOtedw3MJ67"
      },
      "source": [
        "We normally need to update our *world model* periodically on all the collected episodes so far, therefore we need to interleave simulation with model training. Since the size of collected episodes will grow over time, we should persist them to disk and use optimized/cacheable data iterators for training. Utility functions in `utils/npz.py` can be used here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OegEDO9xN_Dg"
      },
      "source": [
        "npz.save_dictionaries(episodes, train_data_dir)\n",
        "train_fn(train_data_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyBiiTiHPR8N"
      },
      "source": [
        "%tensorboard --logdir=/tmp/experiment/ --port=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvfgWvYqRMdQ"
      },
      "source": [
        "Now we can evaluate our agent by using the `base_planner` that is noise free."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYohNSFMRXUh"
      },
      "source": [
        "eval_summary_dir = '/tmp/experiment/eval'\n",
        "episode, predictions, score = simulate.simulate(task=task,\n",
        "                                                planner=base_cem,\n",
        "                                                num_episodes=1)\n",
        "scalar_summaries = {'score': score}\n",
        "train_eval.visualize(summary_dir=eval_summary_dir,\n",
        "                     global_step=i,\n",
        "                     episodes=episode,\n",
        "                     predictions=predictions,\n",
        "                     scalars=scalar_summaries)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw0GZ4wuPUxI"
      },
      "source": [
        "%tensorboard --logdir=/tmp/experiment/ --port=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI9ZKITxCX4B"
      },
      "source": [
        "## Off the Shelf Train-Eval Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXIbN_-zCe6W"
      },
      "source": [
        "A utility function named `train_eval_loop` in `loops/train_eval.py` encapsulates training, evaluating, data collection and tensorboard summary writing all in the same place. If this off the shelf functionality is sufficient we recommend using it instead of implementing them from lower level functions as depicted above.\n",
        "![Architecture diagram](https://i.imgur.com/VjHWDhx.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psKf9kNaUAvl"
      },
      "source": [
        "train_episodes_per_iter = 1  # How many training episodes to collect per train/eval iteration\n",
        "eval_every_n_iters = 10  # A single eval episode every n iterations\n",
        "num_iters = 100  # Total number of train/eval iterations\n",
        "data_dir = '/tmp/experiment/loop/data/'\n",
        "model_dir = '/tmp/experiment/loop/model'\n",
        "\n",
        "train_eval.train_eval_loop(task=task,\n",
        "                           train_planner=train_cem,\n",
        "                           eval_planner=base_cem,\n",
        "                           train_fn=train_fn,\n",
        "                           num_train_episodes_per_iteration=train_episodes_per_iter,\n",
        "                           eval_every_n_iterations=eval_every_n_iters,\n",
        "                           num_iterations=num_iters,\n",
        "                           episodes_dir=data_dir,\n",
        "                           model_dir=model_dir\n",
        "                           )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mPOyAYwPYYE"
      },
      "source": [
        "%tensorboard --logdir=/tmp/experiment/ --port=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K-3gnqJfRmA"
      },
      "source": [
        "## Gin Configs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bClQv1DBfbmp"
      },
      "source": [
        "Finally, it is important to note that all of the above functionalities described above are gin configurable. However we need to provide a gin config to correctly instantiate the task, model, planner and other parameters to assemble an experiment. There are example configs in the `configs/` folder.\n",
        "\n",
        "The main binary `bin/train_eval.py` defines two bindings for `model_dir=<output_dir>/model` and `episodes_dir=<output_dir>/episodes` that are populated from commandline argument `output_dir`. Any gin configurable component that might need either `model_dir` or `episodes_dir`, should use bindings like `<component>.<property>=%model_dir` instead of hard-coding it in the gin config. Below is an example gin config to instantiate a PlaNet agent.\n",
        "\n",
        "```\n",
        "import google3.learning.brain.research.world_models.api.agents.planet\n",
        "\n",
        "# Parameters for model:\n",
        "RecurrentStateSpaceModel.frame_size = (64, 64, 3)\n",
        "RecurrentStateSpaceModel.reward_stop_gradient = False\n",
        "RecurrentStateSpaceModel.task = %TASK\n",
        "# Use singleton to inject the same instance of model later on.\n",
        "MODEL = @model/singleton()\n",
        "model/singleton.constructor = @RecurrentStateSpaceModel\n",
        "\n",
        "# Parameters for predict, observe and reset\n",
        "STRATEGY = @strategy/singleton()\n",
        "strategy/singleton.constructor = @tf.distribute.MirroredStrategy\n",
        "create_planet_predict_fn.model = %MODEL  # Singleton reference\n",
        "create_planet_predict_fn.strategy = %STRATEGY\n",
        "create_planet_observe_fn.model = %MODEL\n",
        "create_planet_observe_fn.model_dir = %model_dir\n",
        "create_planet_observe_fn.strategy = %STRATEGY\n",
        "create_planet_reset_fn.model = %MODEL\n",
        "\n",
        "# Parameters for train_fn:\n",
        "create_planet_train_fn.train_steps = 100\n",
        "create_planet_train_fn.batch = 50\n",
        "create_planet_train_fn.duration = 50\n",
        "create_planet_train_fn.learning_rate = 1e-3\n",
        "create_planet_train_fn.model_dir = %model_dir  # Is populated from flags.\n",
        "create_planet_train_fn.model = %MODEL\n",
        "create_planet_train_fn.strategy = %STRATEGY\n",
        "```\n"
      ]
    }
  ]
}